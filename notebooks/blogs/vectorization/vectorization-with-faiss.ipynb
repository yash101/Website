{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7c98f86e-23e8-4907-adf9-bee58a13f0c7",
   "metadata": {},
   "source": [
    "root = 'blog'\n",
    "name = 'embedding-search-with-faiss'\n",
    "page = 1\n",
    "title = 'Embedding Search with FAISS'\n",
    "subtitle = 'How can we search for our vectors at scale?'\n",
    "isPublished = true\n",
    "\n",
    "publishedOn = '2025-04-07T16:00:00.277Z'\n",
    "lastModifiedOn = '2025-04-07T16:00:00.277Z'\n",
    "authors = 'yash101'\n",
    "\n",
    "opengraph-image = '/assets/objects/blogs/vectorization/opengraph.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b9936-4fec-40d2-b41e-88278b27a58f",
   "metadata": {},
   "source": [
    "In the previous article, we learned how we can test for similarity between two pieces of data through vectorizing and comparing vectors. However, we left off with a linear search to test for vector similarity. This does not scale since our workload increases linearly as the amount of content gets added.\n",
    "\n",
    "In this article, we will use FAISS (Facebook AI Similarity Search) to optimize our vector search. FAISS was built by the legends at Meta and was developed to be capable of handling large scale vector searches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8291b-84a1-4f56-b832-37dd343167eb",
   "metadata": {},
   "source": [
    "## What is FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b2291-a856-4566-a0f6-f5f6e4b1df33",
   "metadata": {},
   "source": [
    "## Article Structure\n",
    "\n",
    "This article is a code-focused article showing how to use FAISS to index and retrieve $k-$nearest vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca22f8-2d69-416f-977b-aafcadc1cfad",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To get started, first ensure you have FAISS installed (`faiss-cpu` if you do not have a GPU, and `faiss-gpu` otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "028da459-95b5-40e6-8ad6-161a01115935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Requirement already satisfied: faiss-cpu in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (1.10.0)',\n",
       " 'Requirement already satisfied: matplotlib in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (3.10.0)',\n",
       " 'Requirement already satisfied: numpy in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (2.2.2)',\n",
       " 'Requirement already satisfied: packaging in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from faiss-cpu) (24.2)',\n",
       " 'Requirement already satisfied: contourpy>=1.0.1 in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from matplotlib) (1.3.1)',\n",
       " 'Requirement already satisfied: cycler>=0.10 in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from matplotlib) (0.12.1)',\n",
       " 'Requirement already satisfied: fonttools>=4.22.0 in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from matplotlib) (4.55.8)',\n",
       " 'Requirement already satisfied: kiwisolver>=1.3.1 in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from matplotlib) (1.4.8)',\n",
       " 'Requirement already satisfied: pillow>=8 in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from matplotlib) (11.1.0)',\n",
       " 'Requirement already satisfied: pyparsing>=2.3.1 in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from matplotlib) (3.2.1)',\n",
       " 'Requirement already satisfied: python-dateutil>=2.7 in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)',\n",
       " 'Requirement already satisfied: six>=1.5 in /Users/yash/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)',\n",
       " '',\n",
       " '\\x1b[1m[\\x1b[0m\\x1b[34;49mnotice\\x1b[0m\\x1b[1;39;49m]\\x1b[0m\\x1b[39;49m A new release of pip is available: \\x1b[0m\\x1b[31;49m24.3.1\\x1b[0m\\x1b[39;49m -> \\x1b[0m\\x1b[32;49m25.0.1\\x1b[0m',\n",
       " '\\x1b[1m[\\x1b[0m\\x1b[34;49mnotice\\x1b[0m\\x1b[1;39;49m]\\x1b[0m\\x1b[39;49m To update, run: \\x1b[0m\\x1b[32;49mpip install --upgrade pip\\x1b[0m']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% hideoutput\n",
    "# Installing Python deps\n",
    "!! pip install faiss-cpu matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1322a-b7b2-4dea-a366-2dea7de856cb",
   "metadata": {},
   "source": [
    "## Next, we can import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "747b985b-e7b5-40a9-aab0-134030035a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7477208-c560-4dc1-999a-b8a7e3864586",
   "metadata": {},
   "source": [
    "## Next, we can load some vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fb492-87c9-4bee-bece-16abf217aeef",
   "metadata": {},
   "source": [
    "**Source for `aesop-vectors.json`:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd6e3081-60d1-4cc3-8f8f-61aa73d14da5",
   "metadata": {},
   "source": [
    "(() => {\n",
    "    return {\n",
    "        tool: 'monaco-preview',\n",
    "        props: {\n",
    "            sourceUrl: '/assets/objects/blogs/vectorization/aesop-vectors.json',\n",
    "            language: 'json'\n",
    "        }\n",
    "    }\n",
    "})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57117f1b-5800-4caa-ac3b-161123aa1ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors: 310; Vector size: 32\n"
     ]
    }
   ],
   "source": [
    "aesop_vectors = None\n",
    "with open('./aesop-vectors.json', 'r') as f:\n",
    "    aesop_vectors = json.load(f)\n",
    "\n",
    "N_DIMENSIONS = len(next(iter(aesop_vectors.values())))\n",
    "\n",
    "print(f'Number of vectors: {len(aesop_vectors)}; Vector size: {N_DIMENSIONS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6fe69-8f6b-43ff-b311-75f8e2fae974",
   "metadata": {},
   "source": [
    "## Next, we can create an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48b9ce77-72b3-41ef-9bc0-f98510fabf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(N_DIMENSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f456d1a-61be-44b1-b89d-6142cae24dee",
   "metadata": {},
   "source": [
    "But wait a moment, what is `IndexFlatL2`?\n",
    "* L2, L2 distance is the Euclidean distance between the two vectors. The distance is calculated by calculating the length of a direct line from one vector to the next.\n",
    "* Flat index is a brute-force index. This works by iterating through every single vector in the database and finding the lowest distances.\n",
    "\n",
    "How can we improve this?\n",
    "\n",
    "1. We can normalize our vectors then use `IndexFlatIP` (inner-product) for cosine similarity\n",
    "2. A better and more efficient indexing strategy? We will look into this further later in this article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0def7ce-5688-4fb1-9a75-97b6c4cfe7a0",
   "metadata": {},
   "source": [
    "## Next, we can add data to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "940d4d06-01b7-476a-a046-fbf0b195d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors:  [[1. 0. 1. ... 1. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "First 10 vectors:  [(0, 'The Wolf And The Lamb'), (1, 'The Bat And The Weasels'), (2, 'The Ass And The Grasshopper'), (3, 'The Lion And The Mouse'), (4, 'The Charcoal-Burner And The Fuller'), (5, 'The Father And His Sons'), (6, 'The Boy Hunting Locusts'), (7, 'The Cock and the Jewel'), (8, 'The Kingdom of the Lion'), (9, 'The Wolf and the Crane')] ...\n"
     ]
    }
   ],
   "source": [
    "vectors = np.array(list(aesop_vectors.values()), dtype='float32')\n",
    "print('Vectors: ', vectors)\n",
    "index.add(vectors)\n",
    "\n",
    "ids_start = index.ntotal - len(vectors)\n",
    "ids_end = index.ntotal\n",
    "id_mapping = {}\n",
    "iter_label = iter(aesop_vectors.keys())\n",
    "for i in range(ids_start, ids_end):\n",
    "    id_mapping[i] = next(iter_label)\n",
    "\n",
    "print('First 10 vectors: ', list(id_mapping.items())[:10], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f9b06-f291-47ea-a3ae-346a1885bb77",
   "metadata": {},
   "source": [
    "## Next, we can query the index to find similar content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a291491c-a71e-4f19-93ca-e20398dca5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fox and the Bramble = [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "Finding k=3 nearest embeddings\n"
     ]
    }
   ],
   "source": [
    "seed_story = 'The Fox and the Bramble'\n",
    "seed_embedding = aesop_vectors[seed_story]\n",
    "print(f'{seed_story} = {seed_embedding}')\n",
    "\n",
    "k = 3\n",
    "print(f'Finding k={k} nearest embeddings')\n",
    "\n",
    "# Query from the FAISS index\n",
    "distances, indices = index.search(np.array([seed_embedding]), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96e31fe4-7383-4d19-8cf3-973da0f74a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for The Fox and the Bramble:\n",
      "   [1] Found The Fox and the Bramble with id 283 with a distance of 0.0\n",
      "   [2] Found The Fox and the Monkey with id 67 with a distance of 1.0\n",
      "   [3] Found The Playful Ass with id 111 with a distance of 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Results for {seed_story}:')\n",
    "for (i, (distance, index)) in enumerate(zip(distances[0], indices[0])):\n",
    "    print(f'   [{i + 1}] Found {id_mapping[index]} with id {index} with a distance of {distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3568ba-1a38-4b88-a0bb-601e2279f99e",
   "metadata": {},
   "source": [
    "## Better Indexing Strategies\n",
    "\n",
    "Our current approach results in us holding every single vector in memory and linearly searching through the vectors to find our best matches. This does not scale due to two reasons:\n",
    "\n",
    "1) We can only fit only so many vectors in RAM\n",
    "2) Linear search is slow, and gets linearly slower as more data is added. No bueno!\n",
    "\n",
    "So what are better indexing methods we can use?\n",
    "\n",
    "\n",
    "See the full list of supported indexing methods at [the FAISS docs, linked here](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028d4fe-cec0-4f4e-a6b9-be1d12ef9ac8",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "* [FAISS docs](https://faiss.ai/)\n",
    "* [FAISS: faiss indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
