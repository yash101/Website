{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d4f1ae-747a-4204-b676-2932998aadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "author = 'Devyash Lodha'\n",
    "title = 'Building a Search Engine - Part 5'\n",
    "lastModified = '2025-02-09T20:56:17.277Z'\n",
    "published = '2035-02-08T21:56:17.277Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafeb18c-1d8e-4b32-aa2e-c5ba8b426107",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55870a36-f54d-424d-b761-3389cba09f49",
   "metadata": {},
   "source": [
    "## Reverse Index Optimization | Can we do Better?\n",
    "\n",
    "Reverse indexes are sort of the perfect search engine. They know everything and if a word exists in a document somewhere, the reverse index will have it. But the problem is that they don't scale! A reverse index may become untenable once holding a couple thousand documents. So how can we optimize a reverse index and shrink it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f4de8-077e-4280-a513-4febf9ce7c0c",
   "metadata": {},
   "source": [
    "### Not all Words are Equal\n",
    "\n",
    "If we take a look at the inverse index created by our naive approach, we see that there is a lot of junk and useless words.\n",
    "\n",
    "Moreover, in most languages such as English, we use lots of words which add little meaning. Just in the last sentence, think abuot what value the following words add: { moreover, in, most, such, as, we, use, lots, of }. What can we do to prune our reverse index and only store what matters?\n",
    "\n",
    "However, the elephant in the room - our preprocessing algorithm is still not as good as necessary. Example: `</o=enron/ou=na/cn=recipients/cn=dl-ga-all_enron_north_america>`. Below, play around with my updated regexps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61871f-81a0-413e-ac14-6ace67f20a17",
   "metadata": {},
   "source": [
    "\n",
    "## Thoughts scratchpad\n",
    "\n",
    "\n",
    "\n",
    "English and most languages are quite wordy. We can remove a lot of the common words since they add little value to the context. But how do we know what value a word adds to the context? There are a few different methods that we can use:\n",
    "\n",
    "* **TF-IDF (Term Frequency-Inverse Document Frequency)**: among the simplest of the ranking functions. Many of the more sophisticated ranking functions are variants of this.\n",
    "* **BM25 (Okapi BM25)**: refined version of TF-IDF, which adds saturation and frequency normalization.\n",
    "* **Word Embeddings (Word2vec, GloVe, FastText)**: capture semantics instead of just term frequency, but more expensive to compute\n",
    "* **Entropy-Based Weighting**: measures how uniformly a word appears in documents. Words appearing across more documents are likely less important\n",
    "\n",
    "There are many more methods, some even involving machine learning. However, for simplicity, we will use TF-IDF in this project, and may switch it out for a better method later on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
