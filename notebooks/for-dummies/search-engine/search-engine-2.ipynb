{
 "cells": [
  {
   "cell_type": "raw",
   "id": "88f6bc9b-baba-4fd2-939a-c6163de7c3e2",
   "metadata": {},
   "source": [
    "root = 'for-dummies'\n",
    "name = 'search-engines'\n",
    "page = 2\n",
    "isPublished = true\n",
    "authors = 'yash101'\n",
    "title = 'Search Engines'\n",
    "subtitle = 'Data acquisition, Cleanup and Tokenization'\n",
    "\n",
    "lastModifiedOn = '2025-02-07T20:56:17.277Z'\n",
    "publishedOn = '2025-02-07T00:00:00.277Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f75b78-bdb3-4286-9893-0a135af0a8c3",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "Data acquisition is arguable the hardest part in building a search engine. In typical search engines, such as Google or Bing, data acquisiton is performed by a **spider**, which **crawls** the web, ingesting content which can be indexed. For this project, we will focus on keeping data acquition simple and use a prepared dataset instead.\n",
    "\n",
    "Since we are using the [Enron Emails Dataset](https://www.cs.cmu.edu/~enron/), to read all our text and build our **corpus**, we simply need to recursively iterate through the directory of the Enron emails dataset and read the files in it. The files are formatted as emails (with headers), but we will ignore that for now unless that turns into an issue (keeping it simple first).\n",
    "\n",
    "```javascript\n",
    "// These functions limit the simultaneous reading of files to prevent exhausting system resouce limits\n",
    "const limit = limitFunction(100);\n",
    "async function limitedReadFile(path) {\n",
    "  return limit(() => fs.readFile(path, 'utf8'));\n",
    "}\n",
    "\n",
    "async function* readFilesRecursively(dir) {\n",
    "  const entries = await fs.readdir(dir, { withFileTypes: true });\n",
    "\n",
    "  for (const entry of entries) {\n",
    "    const fullPath = path.join(dir, entry.name);\n",
    "    if (await entry.isDirectory()) {\n",
    "      yield* readFilesRecursively(fullPath); // Recurse into subdirectory\n",
    "    } else if (entry.isFile()) {\n",
    "      const contents = await limitedReadFile(fullPath);\n",
    "      yield [fullPath, contents];\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "(async () => {\n",
    "  for await (const [ filepath, contents ] of readFilesRecursively(dirPath)) {\n",
    "    // DO something with the filepath and the contents of the file\n",
    "  }\n",
    "})();\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3325b3-12c4-49fd-927a-17c5eedf7edc",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "\n",
    "Data is rarely clean. Common issues faced with search: weird characters, weird spacing, case folding, plenty of Unicode shenanigans (it's a story for another day), different language formats, and so much more. In this article, we will use **Unicode normalization** and a few **regular expressions** to significantly clean up our data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "88cd7bf0-6384-49e0-a25a-fe794d6313cf",
   "metadata": {},
   "source": [
    "(() => {\n",
    "    return {\n",
    "        tool: 'code',\n",
    "        props: {\n",
    "            sourceUrl: '/assets/objects/for-dummies/search-engine/p2-cleanup.js',\n",
    "            autorun: true,\n",
    "            header: false,\n",
    "        }\n",
    "    }\n",
    "})()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e9406-8dff-49ee-85f2-cd54a699ad9d",
   "metadata": {},
   "source": [
    "### Next, Lets Tokenize our Cleaned Data\n",
    "\n",
    "*What is a **Token**?* Tokens are chunks of text which we treat as a single unit - a word, for example. If a document is a Lego castle, each brick used to build that castle is a token.\n",
    "\n",
    "In this article, tokens will be characters separated by spaces - in other words, a *word*."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b75acbf6-a924-4488-8eee-5ca837c052a2",
   "metadata": {},
   "source": [
    "(() => {\n",
    "    return {\n",
    "        tool: 'code',\n",
    "        props: {\n",
    "            defaultSource: `const input = 'hello, world! lorem ipsum';\n",
    "console.log(JSON.stringify(tokenize(input)));\n",
    "\n",
    "function tokenize(string) {\n",
    "  return string\n",
    "    .split(' ')\n",
    "    .filter(str => str && str.length > 0);\n",
    "}\n",
    "`,\n",
    "            autorun: true,\n",
    "            header: false,\n",
    "        }\n",
    "    }\n",
    "})()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82b4cb-4378-46dd-9c1e-9100d620ffdd",
   "metadata": {},
   "source": [
    "### Putting it all Together"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1946fb54-a3ed-4e1d-a6fc-8a2c2f70d2af",
   "metadata": {},
   "source": [
    "(() => {\n",
    "    return {\n",
    "        tool: 'code',\n",
    "        props: {\n",
    "            sourceUrl: '/assets/objects/for-dummies/search-engine/p2-all.js',\n",
    "            autorun: true,\n",
    "            header: false,\n",
    "        }\n",
    "    }\n",
    "})()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a7da3-7552-4eff-a5ca-d57e33886a28",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "In the next section, we will learn how we can intuitively attempt to solve the search problem through the use of a reverse index."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
