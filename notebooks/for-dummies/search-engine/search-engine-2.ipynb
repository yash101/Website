{
 "cells": [
  {
   "cell_type": "raw",
   "id": "88f6bc9b-baba-4fd2-939a-c6163de7c3e2",
   "metadata": {},
   "source": [
    "root = 'for-dummies'\n",
    "name = 'search-engines'\n",
    "page = 2\n",
    "isPublished = true\n",
    "authors = 'yash101'\n",
    "title = 'Search Engines'\n",
    "subtitle = 'Data acquisition, normalization, cleaning and tokenization'\n",
    "\n",
    "lastModifiedOn = '2025-02-07T20:56:17.277Z'\n",
    "publishedOn = '2025-02-07T00:00:00.277Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a74fc2-492f-478d-8e7e-7bda56f8c8be",
   "metadata": {},
   "source": [
    "Search engines are ubiquitous. Google, Bing, Yahoo, you name it, all of these are internet-wide search engines. Even your favorite social media platforms have a search functionality. How do they work? How can we build our own? And how can we build a search engine which is cheap and easy to run?\n",
    "\n",
    "In this part of the multi-part series, we will implement data acquisition and cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f75b78-bdb3-4286-9893-0a135af0a8c3",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "Data acquisition is arguable the hardest part in building a search engine. In typical search engines, such as Google or Bing, data acquisiton is performed by a **spider**, which **crawls** the web, ingesting content which can be indexed. For this project, we will focus on keeping data acquition simple and use a prepared dataset instead.\n",
    "\n",
    "In this project, we will use the [Enron Emails Dataset](https://www.cs.cmu.edu/~enron/). To read all our text and build our **corpus**, we simply need to recursively iterate through the directory of the Enron emails dataset and read the files in it. The files are formatted as emails (with headers), but we will ignore that for now unless that turns into an issue (keeping it simple first).\n",
    "\n",
    "```javascript\n",
    "// These functions limit the simultaneous reading of files to prevent exhausting system resouce limits\n",
    "const limit = limitFunction(100);\n",
    "async function limitedReadFile(path) {\n",
    "  return limit(() => fs.readFile(path, 'utf8'));\n",
    "}\n",
    "\n",
    "async function* readFilesRecursively(dir) {\n",
    "  const entries = await fs.readdir(dir, { withFileTypes: true });\n",
    "\n",
    "  for (const entry of entries) {\n",
    "    const fullPath = path.join(dir, entry.name);\n",
    "    if (await entry.isDirectory()) {\n",
    "      yield* readFilesRecursively(fullPath); // Recurse into subdirectory\n",
    "    } else if (entry.isFile()) {\n",
    "      const contents = await limitedReadFile(fullPath);\n",
    "      yield [fullPath, contents];\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "(async () => {\n",
    "  for await (const [ filepath, contents ] of readFilesRecursively(dirPath)) {\n",
    "    // DO something with the filepath and the contents of the file\n",
    "  }\n",
    "})();\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3325b3-12c4-49fd-927a-17c5eedf7edc",
   "metadata": {},
   "source": [
    "### Data Cleanup and Preprocessing\n",
    "\n",
    "Data is rarely clean. Common issues faced with search: weird characters, weird spacing, case folding, plenty of Unicode shenanigans (it's a story for another day), different language formats, and so much more. In this article, we will use **Unicode normalization** and a few **regular expressions** to significantly clean up our data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "88cd7bf0-6384-49e0-a25a-fe794d6313cf",
   "metadata": {},
   "source": [
    "(() => {\n",
    "    return {\n",
    "        tool: 'code',\n",
    "        props: {\n",
    "            defaultSource: `// https://unicode.org/reports/tr15/\n",
    "function preprocess(string) {\n",
    "  return string\n",
    "    .normalize('NFD')                       // Normalization form, canonical decomposition\n",
    "    // Remove: marks / diacritics, emoji modifiers, punctuation, ZWJ\n",
    "    .replace(/(\\\\p{M}|\\\\p{Emoji_Modifier}|\\\\p{P}|\\\\p{Sc}|\\\\p{Join_Control})/gu, '')\n",
    "    .replace(/(\\\\p{Emoji})/gu, ' $1 ')       // put spaces around emojis so we treat them as words\n",
    "    .replace(/\\\\p{White_Space}/gu, ' ')      // transform whitespace to spaces\n",
    "    .replace(/(\\\\p{Ll})(\\\\p{Lu})/gu, '$1 $2') // split camelCase\n",
    "    .replace(/(\\\\p{N})(\\\\p{L})/gu, '$1 $2')   // split number followed by word without space\n",
    "    .replace(/(\\\\p{L})(\\\\p{N})/gu, '$1 $2')   // split word followed by number without space\n",
    "    .replace(/\\\\s+/gu, ' ')                  // remove extra whitespace between\n",
    "    .toLowerCase();                         // make all text lowercase\n",
    "}\n",
    "\n",
    "const input = 'rÃ©sumÃ© Ä°STANBUL ï¬ancÃ©e ðŸ™ðŸšðŸ› Â½ Â¾ Â¼ ðŸ‘©â€ðŸš€ðŸ¾ðŸ³ï¸â€ðŸŒˆðŸš´ðŸ½â€â™‚ï¸âœŒðŸ¿ search2023update multi-wordâ€”hyphenated excessive spaces FULLWIDTHï¼´ï¼¥ï¼¸ï¼´ ð’žð’½ð‘’ð“‚ð’¾ð’¸ð’¶ð“ Ù…ÙŽØ±Ù’Ø­ÙŽØ¨Ù‹Ø§ ä¸­æ–‡æµ‹è¯• ðŸ’–âœ¨Unicodeâœ¨ðŸ’– HelloðŸ‘©â€ðŸ’»World 2023å¹´æœ€æ–°æƒ…å ± à¤ªà¤°à¥€à¤•à¥à¤·à¤£ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚ Ð´Ð¾Ð±Ñ€Ð¾Ðµ ÑƒÑ‚Ñ€Ð¾ à¤—à¤£à¤¨à¤¾123à¤¶à¤¬à¥à¤¦ '\n",
    "\n",
    "console.log('Before: ', input);\n",
    "console.log('After: ', preprocess(input));\n",
    "`,\n",
    "            autorun: true,\n",
    "            header: false,\n",
    "        }\n",
    "    }\n",
    "})()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e9406-8dff-49ee-85f2-cd54a699ad9d",
   "metadata": {},
   "source": [
    "### Next, Lets Tokenize our Cleaned Data\n",
    "\n",
    "> What is a **Token**? Tokens are chunks of text which we treat as a single unit - a word, for example. If a document is a Lego castle, each brick used to build that castle is a token.\n",
    "\n",
    "In this article, tokens will be characters separated by spaces. In lamen terms, a word."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b75acbf6-a924-4488-8eee-5ca837c052a2",
   "metadata": {},
   "source": [
    "(() => {\n",
    "    return {\n",
    "        tool: 'code',\n",
    "        props: {\n",
    "            defaultSource: `const input = 'hello, world! lorem ipsum';\n",
    "console.log(JSON.stringify(tokenize(input)));\n",
    "\n",
    "function tokenize(string) {\n",
    "  return string\n",
    "    .split(' ')\n",
    "    .filter(str => str && str.length > 0);\n",
    "}\n",
    "`,\n",
    "            autorun: true,\n",
    "            header: false,\n",
    "        }\n",
    "    }\n",
    "})()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82b4cb-4378-46dd-9c1e-9100d620ffdd",
   "metadata": {},
   "source": [
    "### Putting it all Together"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1946fb54-a3ed-4e1d-a6fc-8a2c2f70d2af",
   "metadata": {},
   "source": [
    "(() => {\n",
    "    return {\n",
    "        tool: 'code',\n",
    "        props: {\n",
    "            defaultSource: `const input = 'rÃ©sumÃ© Ä°STANBUL ï¬ancÃ©e ðŸ™ðŸšðŸ› Â½ Â¾ Â¼ ðŸ‘©â€ðŸš€ðŸ¾ðŸ³ï¸â€ðŸŒˆðŸš´ðŸ½â€â™‚ï¸âœŒðŸ¿ search2023update multi-wordâ€”hyphenated excessive spaces FULLWIDTHï¼´ï¼¥ï¼¸ï¼´ ð’žð’½ð‘’ð“‚ð’¾ð’¸ð’¶ð“ Ù…ÙŽØ±Ù’Ø­ÙŽØ¨Ù‹Ø§ ä¸­æ–‡æµ‹è¯• ðŸ’–âœ¨Unicodeâœ¨ðŸ’– HelloðŸ‘©â€ðŸ’»World 2023å¹´æœ€æ–°æƒ…å ± à¤ªà¤°à¥€à¤•à¥à¤·à¤£ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚ Ð´Ð¾Ð±Ñ€Ð¾Ðµ ÑƒÑ‚Ñ€Ð¾ à¤—à¤£à¤¨à¤¾123à¤¶à¤¬à¥à¤¦ '\n",
    "\n",
    "console.log('Before: ', input);\n",
    "console.log('Trans: ', preprocess(input));\n",
    "console.log('Word list:', tokenize(preprocess(input)));\n",
    "\n",
    "function preprocess(string) {\n",
    "  return string\n",
    "    .normalize('NFD')                       // Normalization form, canonical decomposition\n",
    "    // Remove: marks / diacritics, emoji modifiers, punctuation, ZWJ\n",
    "    .replace(/(\\\\p{M}|\\\\p{Emoji_Modifier}|\\\\p{P}|\\\\p{Sc}|\\\\p{Join_Control})/gu, '')\n",
    "    .replace(/(\\\\p{Emoji})/gu, ' $1 ')       // put spaces around emojis so we treat them as words\n",
    "    .replace(/\\\\p{White_Space}/gu, ' ')      // transform whitespace to spaces\n",
    "    .replace(/(\\\\p{Ll})(\\\\p{Lu})/gu, '$1 $2') // split camelCase\n",
    "    .replace(/(\\\\p{N})(\\\\p{L})/gu, '$1 $2')   // split number followed by word without space\n",
    "    .replace(/(\\\\p{L})(\\\\p{N})/gu, '$1 $2')   // split word followed by number without space\n",
    "    .replace(/\\\\s+/gu, ' ')                  // remove extra whitespace between\n",
    "    .toLowerCase();                         // make all text lowercase\n",
    "}\n",
    "\n",
    "function tokenize(string) {\n",
    "  return string\n",
    "    .split(' ')\n",
    "    .filter(str => str && str.length > 0);\n",
    "}`,\n",
    "            autorun: true,\n",
    "            header: false,\n",
    "        }\n",
    "    }\n",
    "})()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a7da3-7552-4eff-a5ca-d57e33886a28",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "In the next section, we will learn how we can intuitively attempt to solve the search problem through the use of a reverse index."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
